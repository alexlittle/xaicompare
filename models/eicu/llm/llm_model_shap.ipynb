{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# LLM Model\n",
    "\n",
    "Using ollama and Gemma"
   ],
   "id": "e085ec7b8007873b"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# --- Setup ---\n",
    "import numpy as np\n",
    "import shap\n",
    "from ollama import Client\n",
    "\n",
    "# 1) Connect to local Ollama\n",
    "#    Default host is http://localhost:11434, change if needed.\n",
    "client = Client()  # Client(host='http://localhost:11434')\n",
    "\n",
    "# 2) Map your diagnosis classes to single-character IDs\n",
    "#    Keep it short and one-token (ASCII uppercase letters are safe).\n",
    "class_names = list(le_y.classes_)  # or your own list of labels\n",
    "assert len(class_names) <= 20, \"Example uses up to 20 classes (A..T). Increase mapping if needed.\"\n",
    "\n",
    "id2char = [chr(ord('A') + i) for i in range(len(class_names))]\n",
    "char2id = {c: i for i, c in enumerate(id2char)}\n",
    "label2char = {lbl: id2char[i] for i, lbl in enumerate(class_names)}\n",
    "\n",
    "# 3) Build a prompt template that forces a one-letter answer\n",
    "def make_prompt(text, classes=id2char, names=class_names):\n",
    "    legend = \"\\n\".join(f\"{c} = {name}\" for c, name in zip(classes, names))\n",
    "    return (\n",
    "        \"You are a medical text classifier.\\n\"\n",
    "        \"Given the patient context below, choose the best diagnosis label.\\n\"\n",
    "        f\"Answer ONLY with one letter from this list and nothing else: {', '.join(classes)}\\n\\n\"\n",
    "        \"Legend:\\n\"\n",
    "        f\"{legend}\\n\\n\"\n",
    "        \"Patient context:\\n\"\n",
    "        f\"{text}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "# 4) The probability function SHAP will call\n",
    "def ollama_predict_proba(texts, model_name=\"gemma3\", top_logprobs=None):\n",
    "    \"\"\"\n",
    "    Returns array of shape (n_samples, n_classes) with probabilities for the one-letter class IDs.\n",
    "    Uses Ollama /api/generate with logprobs for the FIRST generated token only (num_predict=1).\n",
    "    \"\"\"\n",
    "    n_classes = len(id2char)\n",
    "    if top_logprobs is None:\n",
    "        top_logprobs = max(n_classes, 5)  # ensure we capture all candidate letters\n",
    "\n",
    "    out = np.zeros((len(texts), n_classes), dtype=np.float64)\n",
    "\n",
    "    for i, t in enumerate(texts):\n",
    "        prompt = make_prompt(t)\n",
    "\n",
    "        # Single-token completion; deterministic; request logprobs & the top alternatives\n",
    "        resp = client.generate(\n",
    "            model=model_name,\n",
    "            prompt=prompt,\n",
    "            stream=False,\n",
    "            options={\n",
    "                \"temperature\": 0.0,\n",
    "                \"num_predict\": 1,\n",
    "                \"top_k\": 100,     # broad shortlist\n",
    "                \"top_p\": 1.0,\n",
    "            },\n",
    "            logprobs=True,\n",
    "            top_logprobs=top_logprobs,\n",
    "        )\n",
    "\n",
    "        # Parse logprobs for first generated token\n",
    "        # The client returns a dict with 'response' and 'logprobs' per token.\n",
    "        # We expect exactly one generated token (num_predict=1).\n",
    "        logprobs_list = resp.get(\"logprobs\", [])\n",
    "        if not logprobs_list:\n",
    "            # Fallback: if missing, put mass on the chosen token only\n",
    "            chosen = resp.get(\"response\", \"\").strip()[:1].upper()\n",
    "            if chosen in char2id:\n",
    "                out[i, char2id[chosen]] = 1.0\n",
    "            else:\n",
    "                # If the model didn't follow instructions, spread uniformly\n",
    "                out[i, :] = 1.0 / n_classes\n",
    "            continue\n",
    "\n",
    "        first = logprobs_list[0]\n",
    "        # Build a dict of candidate-letter -> logprob\n",
    "        # 1) include the actually generated token\n",
    "        cand_lp = {}\n",
    "        tok = first.get(\"token\", \"\")\n",
    "        lp = first.get(\"logprob\", None)\n",
    "        if tok and lp is not None and tok in char2id:\n",
    "            cand_lp[tok] = lp\n",
    "\n",
    "        # 2) include top alternatives\n",
    "        for alt in first.get(\"top_logprobs\", []) or []:\n",
    "            tok2 = alt.get(\"token\", \"\")\n",
    "            lp2 = alt.get(\"logprob\", None)\n",
    "            if tok2 in char2id and lp2 is not None:\n",
    "                cand_lp[tok2] = lp2\n",
    "\n",
    "        # If any class letter still missing, assign a very low logprob so softmax doesn't zero them out\n",
    "        very_low = -50.0\n",
    "        logits = np.array([cand_lp.get(c, very_low) for c in id2char], dtype=np.float64)\n",
    "\n",
    "        # Softmax to probabilities\n",
    "        logits -= logits.max()\n",
    "        probs = np.exp(logits)\n",
    "        probs /= probs.sum()\n",
    "\n",
    "        out[i, :] = probs\n",
    "\n",
    "    return out\n",
    "\n",
    "# 5) Build a SHAP explainer for text using the probability function\n",
    "masker = shap.maskers.Text()  # tokenizer-aware masking for text\n",
    "explainer = shap.Explainer(lambda texts: ollama_predict_proba(texts, model_name=\"gemma3\"),\n",
    "                           masker=masker,\n",
    "                           output_names=class_names)\n",
    "\n",
    "# 6) Pick a small, representative background set for SHAP (for speed)\n",
    "X_text_train_small = X_text_train[:50] if len(X_text_train) > 50 else X_text_train\n",
    "shap_values_text = explainer(X_text_test[:50])  # explain a small batch\n",
    "\n",
    "# 7) Visualize for a specific class (e.g., index 0)\n",
    "cls_idx = 0\n",
    "shap.plots.text(shap_values_text[:, :, cls_idx])  # token highlights for that class\n",
    "\n"
   ],
   "id": "51ef3982c6a2fdbf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
